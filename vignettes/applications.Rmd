---
title: "Applications"
author: "Danny Morris"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Applications}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = TRUE,
                      cache = FALSE)
```

# Overview

`outsiders` is an unsupervised outlier detection package for use with standard multidimensional data. The `state.x77` data matrix containing 9 demographic characteristics of US states was used often for algorithm development and evaluation. Making use of the same data set, this vignette will provide code and visual analysis demonstrating methods available in the package.

# Document Preparation

```{r, warning = FALSE}
# document dependencies
library(outsiders)   # devtools::install_github("dannymorris/outsiders") to install
library(dplyr)       # data manipualation (CRAN)
library(tidyr)       # structural formatting and data standardization (CRAN)
library(ggplot2)     # static data visualization (CRAN)

library(knitr)       # document printing (eg tables) (CRAN)
library(kableExtra)  # "kable" (table) formatting (CRAN)

library(QuickR)      # another personal package with utility functions
```

# Demographic Data on US States

```{r}
state.x77 %>%
    kable()
```

### Variable Scaling

The standard variable scaling convention is to subtract the variable mean from each observation and divide by standard deviation. Doing so eliminates the undesirable influence of variables measured on larger and wider scales, such as Population (in thousands) compared to HS Grad (percent between 0 and 100).

$$X scaled = (X - mean(X))  /  sd(X)) $$

```{r}
data_scaled <- scale(state.x77)
```

# Exploratory Univariate Analysis

```{r}
par(mfrow = c(3, 3),
    mar = c(2,2,2,2))

for (i in 1:ncol(state.x77)) {
    plot(density(state.x77[, i]), xlab = "", main = colnames(state.x77)[i])
    points(x = state.x77[, i], y = rep(0, 50), 
           col = QuickR::add_alpha("red", 0.5), pch = 21)
}
```

The density plot matrix shows an interesting combination of skewed, normal, and multi-modal distributions. Outliers are evident in univariate visual analysis and are likely evident in the multivariate sense.

# Multivariate Visualization of Principal Components

```{r, fig.height = 6}
principal_components <- princomp(data_scaled)
summary(principal_components, loadings = T)

pca_scores <- principal_components$scores[, 1:4]

pairs(pca_scores, main = "Principal Components 1-4 Explaining \n88% of Total Variation")
```

Visual analysis of the first four principal components reveals the presence of multivariate outliers along with insights into the spatial arrangement of the data points.

# Assessment of Multivariate Normality

```{r}
chisq_distances <- outsiders::chisq_mvn(data_scaled)

plot(chisq ~ distances, data = chisq_distances, 
     main = "Multivariate Ordered Chi-Squared Distances")
abline(lm(chisq ~ distances, data = chisq_distances), lty = 2)
```

The data appears largely non-normal in the multivariate sense.

# Outlier Detection
<hr>

## Attribute-Wise Learning 

The `also` function implements Attribute-Wise Learning for Scoring Outliers (ALSO), which combines supervised and unsupervised learning to score outliers using dependent variable modeling of all features. For each variable in the original data set, a supervised model (classification or regression) is fit using the remaining variables as predictors. Outlier scores reflect the distance of the predicted value for a given observation to its actual value. Outlier scores are then summed to provide a single score suitable for extreme-value analysis.

### Random Forest

```{r}
also_rand_forest <- outsiders::also(data = data_scaled,
                                    method = randomForest::randomForest,
                                    cross_validate = FALSE,
                                    scores_only = TRUE)
```

We will use this custom function to automate and standardize visualizations of outlier scores again the first four principal components.

```{r}
plot_score <- function(score, formula = Comp.2 ~ Comp.1, data = pca_scores, 
                       low = "grey90", high = "blue", ...) {
    colors <- colorRamp(c(low, high))
    mapping <- rgb(colors(score / max(score)), maxColorValue = 255) 
    
    pairs(pca_scores, pch = 19,
         col = QuickR::add_alpha(mapping, 0.85), ...)
}

plot_score(also_rand_forest, main = "ALSO with Random Forests")
```

### Ordinary Least Squares

```{r}
also_lm <- outsiders::also(data = data_scaled,
                           method = lm,
                           cross_validate = TRUE,
                           n_folds = 10,
                           scores_only = TRUE)

plot_score(also_lm, main = "ALSO with OLS")
```

## Proximity-based Scoring
<hr>

This section will show two variations of k-nearest neighbors.

```{r}
average_5nn <- outsiders::aggregate_knn(data_scaled, fun = mean, k = 5)
harmonic_5nn <- outsiders::aggregate_knn(data_scaled, fun = QuickR::harmonic_mean, 
                                         nrow(data_scaled))
```

```{r}
plot_score(average_5nn, main = "Mean Euclidean Distance\n 5-Nearest Neighbors")
plot_score(harmonic_5nn, main = "Harmonic Mean Euclidean Distance\n N-Nearest Neighbors")
```

Compared to the average Euclidean 5-nearest neightbors outlier score the harmonic mean variation seemingly detects more noise.

## Cluster-Based Scoring

## PCA Rotation Forest

```{r}
pca_forest <- outsiders::pca_bag(data = data_scaled,
                                 outlier_fun = function(x) mahalanobis(x, colMeans(x), cov(x)))

plot_score(pca_forest, main = "50-Bag PCA Rotation Forest Combining\n Mahalanobis Distance Scores")
```